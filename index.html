<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>reveal.js – The HTML Presentation Framework</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<br><br>
					<h2 style="text-transform: none">Depth Estimation and Application</h2>
					<br><br>
					<p>Lijun Wang</p>
					<p>July 1, 2018</p>
				</section>
				<section data-transition="zoom">
					<h2 style="text-transform: none">Topics</h2>
					<br>
					<ul>
						<li class="fragment"> Architecture </li>
						<li class="fragment"> Loss Function </li>
						<li class="fragment"> Training Strategy </li>
						<li class="fragment"> Application on RGB-D </li>
					</ul>	
				</section>
				<section>
					<section>
						<h4 style="text-transform: none"><a href="https://cs.nyu.edu/~deigen/depth/">Depth Map Prediction from a Single Image using a Multi-Scale Deep Network</a></h4>
						<p>By Eigen et al., NIPS 20114</p>
						<img width=90% data-src="fig/eigen.png">
					</section>
					<section>
						<h4 style="text-transform: none">Depth Map Prediction from a Single Image using a Multi-Scale Deep Network</h4>
						<ul>
							<li class="fragment">Much of the error is explained by how well the mean depth is predicted</li>
							<li class="fragment"><span style="color: red">20%</span> relative improvement</li>
							<li class="fragment">Scale invariant loss: 
								<br><br>
								$D(y,y^{*}) = \sum \limits_{i,j} [(\log y_i - \log y_j) - (\log y^*_i - \log y^*_j)]^2$</li>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h4 style="text-transform: none"><a href="https://cs.nyu.edu/~deigen/dnl/">One Multi-Scale Architecture for Multi-Task</a></h4>
						<div class="left" style="float: left; width: 50%">
							<img width=100% data-src="fig/eigen2.png">
						</div>
						<div class="right" style="float: right; width: 50%">
							<div style="position: absolute; top: 20%">
								<small style="text-align: justify">[1] Eigen et al, Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture, ICCV 2015	</small>				
								<br> <br> <br>
								<ul>
									<li class="fragment">Multi-scale architecture</li>
									<li class="fragment">Solve multiple tasks</li>
									<li class="fragment">Scale-invariant loss + Gradient loss</li>
								</ul>
							</div>
						</div>
					</section>
				</section>
				<section>
						<section>
							<h4 style="text-transform: none">Deeper Depth Prediction with Fully Convolutional Residual Networks</h4>
							<small>By Laina et al, IEEE International Conference on 3D Vision 2016</small>
							<img width=100% data-src="fig/laina1.png">
						</section>
						<section>
							<h4 style="text-transform: none"> Faster Up-Convolution</h4>
							<img width=100% data-src="fig/laina2.png">
						</section>
						<section>
								<h4 style="text-transform: none"> Faster Up-Convolution</h4>
								<img width=80% data-src="fig/laina3.png">
						</section>
					</section>
				<section>
					<section>
						<h3 style="text-transform: none">A Two-Stream Network for Depth Estimation</h3>
						<img width=100% data-src="fig/two_stream.png">
						<small>[2] Li et al, A Two-Streamed Network for Estimating Fine-Scaled Depth Maps from Single RGB Images, ICCV 2017</small>
					</section>
					<section>
						<h3 style="text-transform: none">A Two-Stream Network for Depth Estimation</h3>
						<ul>
							<li class="fragment">Set Loss</li>
							<p class="fragment">$L_{\textrm{single}} + \Omega_{\textrm{set}}$</p>
							<li class="fragment">Fusing Depth and Depth Gradient</li>
							<ul>
								<li class="fragment">End-to-end as refinement</li>
								<li class="fragment">Optimization</li>
								<p class="fragment">$D^* = \arg \min \limits_{D} \sum \limits _{p=1}^{N} \phi (D^p - D^p_{est}) \\ + 
									\alpha \sum \limits_{p=1}^{N} [\phi (\nabla_x D^p - G_x^p) + \phi (\nabla_y D^p - G_y^p)]$</p>
							</ul>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<div style="position: relative; top: 300px">
							<h3 style="text-transform: none">How about training data?</h3>
						</div>
					</section>
					<section> 
						<h4 style="text-transform: none">Existing Depth Data Set</h4>
						<table style="top: 200px">
							<thead>
								<tr>
									<th>DataSet</th>
									<th>Statics</th>
									<th>Anotation</th>
									<th>Scene</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td><font size="6">
										<a href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">NYUD-v2</a></font></td>
									<td><font size="6">1449 + 407K raw</font></td>
									<td><font size="6">Depth + Segmentation</font></td>
									<td><font size="6">Indoor</font></td>
								</tr>
								<tr>
									<td><font size="6">
										<a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction">KITTI</a>
									</font></td>
									<td><font size="6">94k frames</font></td>
									<td><font size="6">Depth aligned with raw data</font></td>
									<td><font size="6">Street</font></td>
								</tr>
								<tr>
									<td><font size="6">										
										<a href="http://make3d.cs.cornell.edu/data.html">Make3D</a>
									</font></td>
									<td><font size="6">500 low-resolution</font></td>
									<td><font size="6">Depth</font></td>
									<td><font size="6">Outdoor</font></td>
								</tr>
								<tr>
									<td><font size="5">										
										<a href="http://rgbd.cs.princeton.edu/">SUNRGB-D</a>
									</font></td>
									<td><font size="6">10k</font></td>
									<td><font size="6">Depth, Segmentation, 3D bounding box</font></td>
									<td><font size="6">Indoor</font></td>
								</tr>	
							</tbody>
						</table>
					</section>
					<section data-transition="concave">
						<h4 style="text-transform: none">Drawbacks of Exising Data Sets</h4>
						<ul>
							<li>Very limited in terms of scene variety</li>
							<li>Trained models struggle to generalize across scenes </li>
						</ul>
							<img width="90%" data-src="fig/mega1.png">
					</section>
				</section>
				<section>
					<h3 style="text-transform: none">Solution</h3>
					<br>
					<p style="text-align: left">Different training strategies:</p>
					<ul>
						<li class="fragment">Weakly supervised training</li>
						<li class="fragment">Unsupervised training</li>
						<li class="fragment">Semi-supervised training</li>
						<li class="fragment">Multi-task training</li>
					</ul>
				</section>
				<section>
					<section>
						<h3 style="text-transform: none"><a href="http://www-personal.umich.edu/~wfchen/depth-in-the-wild/">Single-Image Depth Perception in the Wild</a></h3>
						<p> By Chen et al, NIPS 2016 </p>
						<img width="90%" data-src="fig/dpw1.png">
					</section>
					<section>
						<h3 style="text-transform: none">Motivation</h3>
						<p style="position: relative; top: 100px">Increase scene diversity with interenet images.</p>
						<img width="100%" data-src="fig/dpw2.png" style="position: relative; top: 150px">
					</section>
					<section>
						<h3 style="text-transform: none">Challenge: How to Acquire Depth</h3>
						<br><br>
						<p> Humans are better at judging relative depth:</p> 
						<p class="fragment">“Is point A closer than point B?”</p>
						<p class="fragment">A relative depth data set</p>
					</section>
					<section>
						<h3 style="text-transform: none">Data Collection</h3>
						<br>
						<br>
						<ul>
							<li class="fragment">Gather 0.5M images from Flickr</li>
							<li class="fragment">Anotate relative depth for one pair of points per image</li>
							<img width="90%" data-src="fig/dpw3.png" class="fragment">
						</ul>
					</section>
					<section>
						<h3 style="text-transform: none">Learning with Relative Depth</h3>
						<p style="text-align: left">Ranking Loss:</p>
						<p>$L(I,R,z)=\sum \limits_k \psi(I, i_k, j_k, r, z)$</p>
						<p style="text-align: left">where the loss for the $k$-th quiry:</p>
						<p>$\psi(I, i_k, j_k, r, z) =  \begin{cases} \log (1+\exp (-z_{i_k} + z_{j_k})), & \mbox{if } r_k=+1\\  
							\log (1+\exp (z_{i_k} - z_{j_k})), & \mbox{if } r_k=-1 \\
							(z_{i_k} - z_{j_k})^2, & \mbox{if } r_k=0 \end{cases}$</p>
					</section>
					<section>
						<img width="90%" data-src="fig/dpw4.png">
					</section>
				</section>
				<section>
					<section>
						<h3 style="text-transform: none">Generate GT Depth from Multi-view Internet Images</h3>
						<small>[3] MegaDepth: Learning Single-View Depth Prediction from Internet Photos, CVPR 2018, <a href="http://www.cs.cornell.edu/
							projects/megadepth/">WebPage</a></small>
						<ul>
							<li class="fragment">Landmark10k data sets with multi-view photos for each landmark</li>
							<li class="fragment">Build 3D model for each collection with SfM</li>
							<li class="fragment">Depth Reconstruction with MVS</li>
							<img class="fragment" width=100% data-src="fig/mega2.png">
						</ul>
					</section>
					<section>
						<h3 style="text-transform: none">Data Categorization: Euclidean vs. Ordinal Depth </h3>
						<p class="fragment">If $\ge 30\%$ valid depth &#8658 Euclidean loss</p>
						<p class="fragment">Otherwise &#8658 ordinal loss</p>
						<div class="fragment">
							<img width=100% data-src="fig/mega3.png">
							<p>Determine foreground with semantic info</p>
						</div>
					</section>
					<section>
						<h3 style="text-transform: none"> Loss Function </h3>
						$L = L_{\mbox{data}} + \alpha L_{\mbox{grad}} + \beta L_{\mbox{ord}}$ 
						<p class="fragment">$L_{\mbox{grad}}=\frac{1}{n} \sum \limits_k \sum \limits_i 
							(|\nabla_x R_i^k + \nabla_y R_i^k|)$</p>
							<img class="fragment" width=70% data-src="fig/mega4.png">
					</section>
					<section>
						<h3 style="text-transform: none"> A Similar Work </h3>
						<small>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xian_Monocular_Relative_Depth_CVPR_2018_paper.pdf">4</a>] Monocular Relative Depth Perception with Web Stereo Data Supervision, CVPR 2018								</small>
						<ul>
							<li class="fragment">Collect stereo web images for depth estimation</li>
							<li class="fragment">Compute optical flow to infer disparity (depth)</li>
							<li class="fragment">Still use ordinal loss but sample point pairs online</li>
							<span class="fragment">(Absolute depth is unavailable?)</span>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h3 style="text-transform: none"> Learning Depth Estimation from Image Alignment Loss </h3>
						<small>[<a href="https://github.com/Yevkuzn/semodepth">4</a>] Semi-Supervised Deep Learning for Monocular Depth Map Prediction, CVPR 2018</small>
						<div class="left" style="float: left; width: 50%">
							<img width=100% data-src="fig/sd1.png">
						</div>
						<div class="right" style="float: right; width: 50%; position: relative; top: 50px; ">
							<ul>
								<li class="fragment">Supervised: sparse depth supervision</li>
								<li class="fragment">Unsupervised: image alignment</li>
							</ul>
						</div>
					</section>
					<section>
						<h3 style="text-transform: none"> Similar Ideas using Monocular Videos </h3>
						<img width=100% data-src="fig/video1.png">
					</section>
					<section data-transition="none">
						<h3 style="text-transform: none"> Similar Ideas using Monocular Videos </h3>
						<p style="text-align: left"><small>[<a href="https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/">5</a>] Unsupervised Learning of Depth and Ego-Motion from Video, CVPR 2017</small></p>
						<p style="text-align: left"><small>[<a href="https://github.com/yzcjtr/GeoNet">6</a>]GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose, CVPR 2018</small></p>
						<p style="text-align: left"><small>[<a href="https://github.com/MightyChaos/LKVOLearner">7</a>]Learning Depth from Monocular Videos using Direct Methods, CVPR 2018</small></p>
					</section>
				</section>
				<section>
					<section>
						<h4 style="text-transform: none"> <a href="https://github.com/google/aperture_supervision">Aperture Supervision for Monocular Depth Estimation</a> </h4>
						<p>By Srinivasan et al, CVPR 2018</p>
						<p> Image 
							<span class="fragment"> &#8658 Depth</span> 
							<span class="fragment">&#8658 Rendering function </span>
							<span class="fragment">&#8658 Shallow DoF </span></p>
						<p class="fragment">Scenarios are limited, mainly flowers</p>
						<img width="100%" data-src="fig/aperture.png">
					</section>
				</section>
				<section>
					<section>
						<h3 style="text-transform: none"> Additional Works with New Training Strategies</h3>
						<p><small>[<a href="https://arxiv.org/abs/1805.04409">8</a>]PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network
							for Simultaneous Depth Estimation and Scene Parsing, CVPR 18</small></p>
						<img width="100%" data-src="fig/mt.png">
					</section>
					<section data-transition="concave">
						<h3 style="text-transform: none"> Additional Works with New Training Strategies</h3>
						<p><small>[<a href="https://arxiv.org/abs/1803.01599">9</a>]AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimatio, CVPR 18</small></p>
						<img width="90%" data-src="fig/ada1.png">
					</section>
					<section data-transition="concave">
						<h3 style="text-transform: none"> Additional Works with New Training Strategies</h3>
						<p><small>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/4075.pdf">10</a>]Salience Guided Depth Calibration for Perceptually Optimized Compressive
							Light Field 3D Display, CVPR 18</small></p>
						<img width="90%" data-src="fig/sg.png">
					</section>
				</section>
				<section>
					<section>
						<h3 style="text-transform: none">Future Directions</h3>
						<ul>
							<li class="fragment">Borrow idea from saliency for depth estimation</li>
							<li class="fragment">Depth completion/refinement from sparse input</li>
							<li class="fragment">Application based on RGB-D data</li>
						</ul>
					</section>
					<section>
						<h3 style="text-transform: none"> Application on RGB-D</h3>
						
						<ul>
								<span> <font size="5">2D &#8658 2.5D &#8658 3D</font></span>
							<li class="fragment"><font size="5">Using depth as additional low-level cues</font></li>
							<li class="fragment"><font size="5">Solve 2D by 3D construction for better scene understanding</font></li>
							<span class="fragment"><font size="4"><a href="http://web.stanford.edu/class/cs231a/"> CS231A: Computer Vision, From 3D Reconstruction to Recognition</a></font></span>
						</ul>
						<img width="70%" data-src="fig/app1.png">


					</section>
				</section>




				<section>
					<h1>Reveal.js</h1>
					<h3>The HTML Presentation Framework</h3>
					<p>
						<small>Created by <a href="http://hakim.se">Hakim El Hattab</a> / <a href="http://twitter.com/hakimel">@hakimel</a></small>
					</p>
				</section>

				<section>
					<h2>Hello There</h2>
					<p>
						reveal.js enables you to create beautiful interactive slide decks using HTML. This presentation will show you examples of what it can do.
					</p>
				</section>

				<!-- Example of nested vertical slides -->
				<section>
					<section>
						<h2>Vertical Slides</h2>
						<p>Slides can be nested inside of each other.</p>
						<p>Use the <em>Space</em> key to navigate through all slides.</p>
						<br>
						<a href="#" class="navigate-down">
							<img width="178" height="238" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
						</a>
					</section>
					<section>
						<h2>Basement Level 1</h2>
						<p>Nested slides are useful for adding additional detail underneath a high level horizontal slide.</p>
					</section>
					<section>
						<h2>Basement Level 2</h2>
						<p>That's it, time to go back up.</p>
						<br>
						<a href="#/2">
							<img width="178" height="238" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Up arrow" style="transform: rotate(180deg); -webkit-transform: rotate(180deg);">
						</a>
					</section>
				</section>

				<section>
					<h2>Slides</h2>
					<p>
						Not a coder? Not a problem. There's a fully-featured visual editor for authoring these, try it out at <a href="http://slides.com" target="_blank">http://slides.com</a>.
					</p>
				</section>

				<section>
					<h2>Point of View</h2>
					<p>
						Press <strong>ESC</strong> to enter the slide overview.
					</p>
					<p>
						Hold down alt and click on any element to zoom in on it using <a href="http://lab.hakim.se/zoom-js">zoom.js</a>. Alt + click anywhere to zoom back out.
					</p>
				</section>

				<section>
					<h2>Touch Optimized</h2>
					<p>
						Presentations look great on touch devices, like mobile phones and tablets. Simply swipe through your slides.
					</p>
				</section>

				<section data-markdown>
					<script type="text/template">
						## Markdown support

						Write content using inline or external Markdown.
						Instructions and more info available in the [readme](https://github.com/hakimel/reveal.js#markdown).

						```
						<section data-markdown>
						  ## Markdown support

						  Write content using inline or external Markdown.
						  Instructions and more info available in the [readme](https://github.com/hakimel/reveal.js#markdown).
						</section>
						```
					</script>
				</section>

				<section>
					<section id="fragments">
						<h2>Fragments</h2>
						<p>Hit the next arrow...</p>
						<p class="fragment">... to step through ...</p>
						<p><span class="fragment">... a</span> <span class="fragment">fragmented</span> <span class="fragment">slide.</span></p>

						<aside class="notes">
							This slide has fragments which are also stepped through in the notes window.
						</aside>
					</section>
					<section>
						<h2>Fragment Styles</h2>
						<p>There's different types of fragments, like:</p>
						<p class="fragment grow">grow</p>
						<p class="fragment shrink">shrink</p>
						<p class="fragment fade-out">fade-out</p>
						<p class="fragment current-visible">current-visible</p>
						<p class="fragment highlight-red">highlight-red</p>
						<p class="fragment highlight-blue">highlight-blue</p>
					</section>
				</section>

				<section id="transitions">
					<h2>Transition Styles</h2>
					<p>
						You can select from different transitions, like: <br>
						<a href="?transition=none#/transitions">None</a> -
						<a href="?transition=fade#/transitions">Fade</a> -
						<a href="?transition=slide#/transitions">Slide</a> -
						<a href="?transition=convex#/transitions">Convex</a> -
						<a href="?transition=concave#/transitions">Concave</a> -
						<a href="?transition=zoom#/transitions">Zoom</a>
					</p>
				</section>

				<section id="themes">
					<h2>Themes</h2>
					<p>
						reveal.js comes with a few themes built in: <br>
						<!-- Hacks to swap themes after the page has loaded. Not flexible and only intended for the reveal.js demo deck. -->
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/black.css'); return false;">Black (default)</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/white.css'); return false;">White</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/league.css'); return false;">League</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/sky.css'); return false;">Sky</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/beige.css'); return false;">Beige</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/simple.css'); return false;">Simple</a> <br>
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/serif.css'); return false;">Serif</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/blood.css'); return false;">Blood</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/night.css'); return false;">Night</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/moon.css'); return false;">Moon</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/solarized.css'); return false;">Solarized</a>
					</p>
				</section>

				<section>
					<section data-background="#dddddd">
						<h2>Slide Backgrounds</h2>
						<p>
							Set <code>data-background="#dddddd"</code> on a slide to change the background color. All CSS color formats are supported.
						</p>
						<a href="#" class="navigate-down">
							<img width="178" height="238" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
						</a>
					</section>
					<section data-background="https://s3.amazonaws.com/hakim-static/reveal-js/image-placeholder.png">
						<h2>Image Backgrounds</h2>
						<pre><code class="hljs">&lt;section data-background="image.png"&gt;</code></pre>
					</section>
					<section data-background="https://s3.amazonaws.com/hakim-static/reveal-js/image-placeholder.png" data-background-repeat="repeat" data-background-size="100px">
						<h2>Tiled Backgrounds</h2>
						<pre><code class="hljs" style="word-wrap: break-word;">&lt;section data-background="image.png" data-background-repeat="repeat" data-background-size="100px"&gt;</code></pre>
					</section>
					<section data-background-video="https://s3.amazonaws.com/static.slid.es/site/homepage/v1/homepage-video-editor.mp4,https://s3.amazonaws.com/static.slid.es/site/homepage/v1/homepage-video-editor.webm" data-background-color="#000000">
						<div style="background-color: rgba(0, 0, 0, 0.9); color: #fff; padding: 20px;">
							<h2>Video Backgrounds</h2>
							<pre><code class="hljs" style="word-wrap: break-word;">&lt;section data-background-video="video.mp4,video.webm"&gt;</code></pre>
						</div>
					</section>
					<section data-background="http://i.giphy.com/90F8aUepslB84.gif">
						<h2>... and GIFs!</h2>
					</section>
				</section>

				<section data-transition="slide" data-background="#4d7e65" data-background-transition="zoom">
					<h2>Background Transitions</h2>
					<p>
						Different background transitions are available via the backgroundTransition option. This one's called "zoom".
					</p>
					<pre><code class="hljs">Reveal.configure({ backgroundTransition: 'zoom' })</code></pre>
				</section>

				<section data-transition="slide" data-background="#b5533c" data-background-transition="zoom">
					<h2>Background Transitions</h2>
					<p>
						You can override background transitions per-slide.
					</p>
					<pre><code class="hljs" style="word-wrap: break-word;">&lt;section data-background-transition="zoom"&gt;</code></pre>
				</section>

				<section>
					<h2>Pretty Code</h2>
					<pre><code class="hljs" data-trim contenteditable>
function linkify( selector ) {
  if( supports3DTransforms ) {

    var nodes = document.querySelectorAll( selector );

    for( var i = 0, len = nodes.length; i &lt; len; i++ ) {
      var node = nodes[i];

      if( !node.className ) {
        node.className += ' roll';
      }
    }
  }
}
					</code></pre>
					<p>Code syntax highlighting courtesy of <a href="http://softwaremaniacs.org/soft/highlight/en/description/">highlight.js</a>.</p>
				</section>

				<section>
					<h2>Marvelous List</h2>
					<ul>
						<li>No order here</li>
						<li>Or here</li>
						<li>Or here</li>
						<li>Or here</li>
					</ul>
				</section>

				<section>
					<h2>Fantastic Ordered List</h2>
					<ol>
						<li>One is smaller than...</li>
						<li>Two is smaller than...</li>
						<li>Three!</li>
					</ol>
				</section>

				<section>
					<h2>Tabular Tables</h2>
					<table>
						<thead>
							<tr>
								<th>Item</th>
								<th>Value</th>
								<th>Quantity</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>Apples</td>
								<td>$1</td>
								<td>7</td>
							</tr>
							<tr>
								<td>Lemonade</td>
								<td>$2</td>
								<td>18</td>
							</tr>
							<tr>
								<td>Bread</td>
								<td>$3</td>
								<td>2</td>
							</tr>
						</tbody>
					</table>
				</section>

				<section>
					<h2>Clever Quotes</h2>
					<p>
						These guys come in two forms, inline: <q cite="http://searchservervirtualization.techtarget.com/definition/Our-Favorite-Technology-Quotations">
						&ldquo;The nice thing about standards is that there are so many to choose from&rdquo;</q> and block:
					</p>
					<blockquote cite="http://searchservervirtualization.techtarget.com/definition/Our-Favorite-Technology-Quotations">
						&ldquo;For years there has been a theory that millions of monkeys typing at random on millions of typewriters would
						reproduce the entire works of Shakespeare. The Internet has proven this theory to be untrue.&rdquo;
					</blockquote>
				</section>

				<section>
					<h2>Intergalactic Interconnections</h2>
					<p>
						You can link between slides internally,
						<a href="#/2/3">like this</a>.
					</p>
				</section>

				<section>
					<h2>Speaker View</h2>
					<p>There's a <a href="https://github.com/hakimel/reveal.js#speaker-notes">speaker view</a>. It includes a timer, preview of the upcoming slide as well as your speaker notes.</p>
					<p>Press the <em>S</em> key to try it out.</p>

					<aside class="notes">
						Oh hey, these are some notes. They'll be hidden in your presentation, but you can see them if you open the speaker notes window (hit 's' on your keyboard).
					</aside>
				</section>

				<section>
					<h2>Export to PDF</h2>
					<p>Presentations can be <a href="https://github.com/hakimel/reveal.js#pdf-export">exported to PDF</a>, here's an example:</p>
					<iframe src="https://www.slideshare.net/slideshow/embed_code/42840540" width="445" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:3px solid #666; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>
				</section>

				<section>
					<h2>Global State</h2>
					<p>
						Set <code>data-state="something"</code> on a slide and <code>"something"</code>
						will be added as a class to the document element when the slide is open. This lets you
						apply broader style changes, like switching the page background.
					</p>
				</section>

				<section data-state="customevent">
					<h2>State Events</h2>
					<p>
						Additionally custom events can be triggered on a per slide basis by binding to the <code>data-state</code> name.
					</p>
					<pre><code class="javascript" data-trim contenteditable style="font-size: 18px;">
Reveal.addEventListener( 'customevent', function() {
	console.log( '"customevent" has fired' );
} );
					</code></pre>
				</section>

				<section>
					<h2>Take a Moment</h2>
					<p>
						Press B or . on your keyboard to pause the presentation. This is helpful when you're on stage and want to take distracting slides off the screen.
					</p>
				</section>

				<section>
					<h2>Much more</h2>
					<ul>
						<li>Right-to-left support</li>
						<li><a href="https://github.com/hakimel/reveal.js#api">Extensive JavaScript API</a></li>
						<li><a href="https://github.com/hakimel/reveal.js#auto-sliding">Auto-progression</a></li>
						<li><a href="https://github.com/hakimel/reveal.js#parallax-background">Parallax backgrounds</a></li>
						<li><a href="https://github.com/hakimel/reveal.js#keyboard-bindings">Custom keyboard bindings</a></li>
					</ul>
				</section>

				<section style="text-align: left;">
					<h1>THE END</h1>
					<p>
						- <a href="http://slides.com">Try the online editor</a> <br>
						- <a href="https://github.com/hakimel/reveal.js">Source code &amp; documentation</a>
					</p>
				</section>

			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: false,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math/math.js', async: true}
				]
			});

		</script>

	</body>
</html>
